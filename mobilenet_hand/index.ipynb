{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab07f3f0",
   "metadata": {},
   "source": [
    "# ==============================\n",
    "# 1. SETUP\n",
    "# =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b0a0c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mediapipe as mp\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import albumentations as A\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82f91c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.12.0\n",
      "OpenCV version: 4.12.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"OpenCV version:\", cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddbf4db",
   "metadata": {},
   "source": [
    "# ==============================\n",
    "# 2. KONFIGURASI DAN PARAMETER\n",
    "# =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf7fdfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigurasi dataset\n",
    "DATASET_PATH = 'hand_gesture_dataset'\n",
    "CLASS_NAMES = ['maju', 'kanan', 'kiri', 'stop']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Membuat direktori untuk hasil\n",
    "os.makedirs('content/processed_data', exist_ok=True)\n",
    "os.makedirs('content/models', exist_ok=True)\n",
    "os.makedirs('content/results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ffa17",
   "metadata": {},
   "source": [
    "# ==============================\n",
    "# 3. PREPROCESSING DENGAN MEDIAPIPE\n",
    "# =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d892b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.hands = self.mp_hands.Hands(\n",
    "            static_image_mode=True,\n",
    "            max_num_hands=1,\n",
    "            min_detection_confidence=0.7,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "    \n",
    "    def detect_and_crop_hand(self, image_path, output_size=(224, 224)):\n",
    "        \"\"\"Deteksi tangan menggunakan MediaPipe dan crop area tangan\"\"\"\n",
    "        try:\n",
    "            # Baca gambar\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                return None, None\n",
    "            \n",
    "            # Convert BGR to RGB\n",
    "            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Deteksi tangan\n",
    "            results = self.hands.process(rgb_image)\n",
    "            \n",
    "            if results.multi_hand_landmarks:\n",
    "                # Ambil landmark tangan pertama\n",
    "                hand_landmarks = results.multi_hand_landmarks[0]\n",
    "                \n",
    "                # Dapatkan koordinat bounding box\n",
    "                h, w, _ = image.shape\n",
    "                x_coords = [landmark.x * w for landmark in hand_landmarks.landmark]\n",
    "                y_coords = [landmark.y * h for landmark in hand_landmarks.landmark]\n",
    "                \n",
    "                # Hitung bounding box dengan padding\n",
    "                x_min, x_max = int(min(x_coords)), int(max(x_coords))\n",
    "                y_min, y_max = int(min(y_coords)), int(max(y_coords))\n",
    "                \n",
    "                # Tambah padding 20%\n",
    "                padding_x = int((x_max - x_min) * 0.2)\n",
    "                padding_y = int((y_max - y_min) * 0.2)\n",
    "                \n",
    "                x_min = max(0, x_min - padding_x)\n",
    "                x_max = min(w, x_max + padding_x)\n",
    "                y_min = max(0, y_min - padding_y)\n",
    "                y_max = min(h, y_max + padding_y)\n",
    "                \n",
    "                # Crop tangan\n",
    "                cropped_hand = image[y_min:y_max, x_min:x_max]\n",
    "                \n",
    "                # Resize ke ukuran target\n",
    "                cropped_hand = cv2.resize(cropped_hand, output_size)\n",
    "                \n",
    "                return cropped_hand, (x_min, y_min, x_max, y_max)\n",
    "            else:\n",
    "                # Jika tidak terdeteksi tangan, gunakan gambar original dengan resize\n",
    "                resized_image = cv2.resize(image, output_size)\n",
    "                return resized_image, None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {str(e)}\")\n",
    "            return None, None\n",
    "\n",
    "def preprocess_dataset():\n",
    "    \"\"\"Preprocessing dataset dengan MediaPipe\"\"\"\n",
    "    preprocessor = HandPreprocessor()\n",
    "    \n",
    "    processed_data = []\n",
    "    labels = []\n",
    "    failed_images = []\n",
    "    \n",
    "    print(\"Memulai preprocessing dataset...\")\n",
    "    \n",
    "    for class_idx, class_name in enumerate(CLASS_NAMES):\n",
    "        class_path = os.path.join(DATASET_PATH, class_name)\n",
    "        if not os.path.exists(class_path):\n",
    "            print(f\"Warning: Path {class_path} tidak ditemukan!\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing class: {class_name}\")\n",
    "        \n",
    "        # Ambil semua file gambar\n",
    "        image_files = []\n",
    "        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:\n",
    "            image_files.extend(glob.glob(os.path.join(class_path, ext)))\n",
    "            image_files.extend(glob.glob(os.path.join(class_path, ext.upper())))\n",
    "        \n",
    "        print(f\"Found {len(image_files)} images in {class_name}\")\n",
    "        \n",
    "        for img_path in image_files:\n",
    "            cropped_image, bbox = preprocessor.detect_and_crop_hand(img_path)\n",
    "            \n",
    "            if cropped_image is not None:\n",
    "                processed_data.append(cropped_image)\n",
    "                labels.append(class_idx)\n",
    "            else:\n",
    "                failed_images.append(img_path)\n",
    "    \n",
    "    print(f\"Preprocessing selesai!\")\n",
    "    print(f\"Total gambar berhasil diproses: {len(processed_data)}\")\n",
    "    print(f\"Total gambar gagal: {len(failed_images)}\")\n",
    "    \n",
    "    # Simpan data yang gagal untuk review\n",
    "    if failed_images:\n",
    "        with open('/content/failed_images.txt', 'w') as f:\n",
    "            for img in failed_images:\n",
    "                f.write(f\"{img}\\n\")\n",
    "    \n",
    "    return np.array(processed_data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b457396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. PREPROCESSING DATA DENGAN MEDIAPIPE\n",
      "----------------------------------------\n",
      "Memulai preprocessing dataset...\n",
      "Processing class: maju\n",
      "Found 600 images in maju\n",
      "Processing class: kanan\n",
      "Found 600 images in kanan\n",
      "Processing class: kiri\n",
      "Found 600 images in kiri\n",
      "Processing class: stop\n",
      "Found 600 images in stop\n",
      "Preprocessing selesai!\n",
      "Total gambar berhasil diproses: 2400\n",
      "Total gambar gagal: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1. PREPROCESSING DATA DENGAN MEDIAPIPE\")\n",
    "print(\"-\"*40)\n",
    "X, y = preprocess_dataset()\n",
    "    \n",
    "if len(X) == 0:\n",
    "    print(\"Error: No data loaded. Please check your dataset path!\")\n",
    "    exit()    \n",
    "    \n",
    "# Normalisasi data\n",
    "X = X.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf5a78",
   "metadata": {},
   "source": [
    "# ==============================\n",
    "# 4. DATA AUGMENTASI\n",
    "# =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78314a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_augmentation_pipeline():\n",
    "    \"\"\"Buat pipeline augmentasi data\"\"\"\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Rotate(limit=15, p=0.7),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.7),\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "        A.MotionBlur(blur_limit=3, p=0.3),\n",
    "        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=0.5),\n",
    "        A.CoarseDropout(max_holes=8, max_height=8, max_width=8, p=0.3),\n",
    "    ], p=1.0)\n",
    "\n",
    "def augment_data(X, y, augment_factor=2):\n",
    "    \"\"\"Augmentasi data untuk meningkatkan variasi\"\"\"\n",
    "    transform = create_augmentation_pipeline()\n",
    "    \n",
    "    augmented_X = []\n",
    "    augmented_y = []\n",
    "    \n",
    "    # Tambahkan data original\n",
    "    augmented_X.extend(X)\n",
    "    augmented_y.extend(y)\n",
    "    \n",
    "    print(\"Melakukan augmentasi data...\")\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        for _ in range(augment_factor):\n",
    "            # Apply augmentation\n",
    "            augmented = transform(image=X[i])\n",
    "            augmented_image = augmented['image']\n",
    "            \n",
    "            augmented_X.append(augmented_image)\n",
    "            augmented_y.append(y[i])\n",
    "    \n",
    "    print(f\"Data original: {len(X)}\")\n",
    "    print(f\"Data setelah augmentasi: {len(augmented_X)}\")\n",
    "    \n",
    "    return np.array(augmented_X), np.array(augmented_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66b15438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. SPLITTING DATA\n",
      "----------------------------------------\n",
      "Train set: 1440 samples\n",
      "Validation set: 480 samples\n",
      "Test set: 480 samples\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2. SPLITTING DATA\")\n",
    "print(\"-\"*40)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a3a7e",
   "metadata": {},
   "source": [
    "# ==============================\n",
    "# 5. MOBILENETV2 SSD MODEL\n",
    "# =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "597b21a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mobilenetv2_model(input_shape=(224, 224, 3), num_classes=5):\n",
    "    \"\"\"Buat model MobileNetV2 dengan SSD-inspired architecture\"\"\"\n",
    "    \n",
    "    # Base model MobileNetV2\n",
    "    base_model = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=input_shape,\n",
    "        alpha=1.0,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # Freeze beberapa layer awal\n",
    "    for layer in base_model.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # SSD-inspired head\n",
    "    inputs = base_model.input\n",
    "    x = base_model.output\n",
    "    \n",
    "    # Multi-scale feature extraction\n",
    "    # Feature map 1: 7x7\n",
    "    feat1 = layers.GlobalAveragePooling2D(name='feat1_gap')(x)\n",
    "    feat1 = layers.Dense(512, activation='relu', name='feat1_dense')(feat1)\n",
    "    \n",
    "    # Feature map 2: dari layer sebelumnya\n",
    "    x_prev = base_model.get_layer('block_13_expand_relu').output  # 14x14\n",
    "    feat2 = layers.GlobalAveragePooling2D(name='feat2_gap')(x_prev)\n",
    "    feat2 = layers.Dense(256, activation='relu', name='feat2_dense')(feat2)\n",
    "    \n",
    "    # Feature map 3: dari layer lebih awal\n",
    "    x_early = base_model.get_layer('block_6_expand_relu').output  # 28x28\n",
    "    feat3 = layers.GlobalAveragePooling2D(name='feat3_gap')(x_early)\n",
    "    feat3 = layers.Dense(128, activation='relu', name='feat3_dense')(feat3)\n",
    "    \n",
    "    # Gabungkan semua features\n",
    "    combined_features = layers.Concatenate(name='combined_features')([feat1, feat2, feat3])\n",
    "    \n",
    "    # Classification head\n",
    "    x = layers.Dropout(0.5, name='dropout1')(combined_features)\n",
    "    x = layers.Dense(512, activation='relu', name='classifier_dense1')(x)\n",
    "    x = layers.Dropout(0.3, name='dropout2')(x)\n",
    "    x = layers.Dense(256, activation='relu', name='classifier_dense2')(x)\n",
    "    \n",
    "    # Output layer\n",
    "    predictions = layers.Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08ed7ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. DATA AUGMENTATION\n",
      "----------------------------------------\n",
      "Melakukan augmentasi data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandi\\AppData\\Local\\Temp\\ipykernel_800\\122538788.py:7: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
      "C:\\Users\\sandi\\AppData\\Local\\Temp\\ipykernel_800\\122538788.py:10: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=8, max_height=8, max_width=8, p=0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data original: 1440\n",
      "Data setelah augmentasi: 4320\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n3. DATA AUGMENTATION\")\n",
    "print(\"-\"*40)\n",
    "X_train_aug, y_train_aug = augment_data(X_train, y_train, augment_factor=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14164fb2",
   "metadata": {},
   "source": [
    "# ==============================\n",
    "# 6. TRAINING PIPELINE\n",
    "# =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66111c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Training model dengan callbacks\"\"\"\n",
    "    \n",
    "    # Create model\n",
    "    model = create_mobilenetv2_model(num_classes=NUM_CLASSES)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"Model Summary:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            '/content/models/best_hand_model.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Training\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot hasil training\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0, 0].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    axes[0, 0].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    axes[0, 0].set_title('Model Accuracy')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 1].plot(history.history['loss'], label='Train Loss')\n",
    "    axes[0, 1].plot(history.history['val_loss'], label='Val Loss')\n",
    "    axes[0, 1].set_title('Model Loss')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Learning rate (jika ada)\n",
    "    if 'lr' in history.history:\n",
    "        axes[1, 0].plot(history.history['lr'], label='Learning Rate')\n",
    "        axes[1, 0].set_title('Learning Rate')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('LR')\n",
    "        axes[1, 0].set_yscale('log')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/content/results/training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f02f70a",
   "metadata": {},
   "source": [
    "# ==============================\n",
    "# 7. EVALUASI DAN METRIK\n",
    "# =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdb6f562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluasi model dengan berbagai metrik\"\"\"\n",
    "    \n",
    "    # Prediksi\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    \n",
    "    # Precision, Recall, F1-score per class\n",
    "    precision = precision_score(y_test, y_pred, average=None)\n",
    "    recall = recall_score(y_test, y_pred, average=None)\n",
    "    f1 = f1_score(y_test, y_pred, average=None)\n",
    "    \n",
    "    # Overall metrics\n",
    "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"HASIL EVALUASI MODEL\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Macro Precision: {precision_macro:.4f}\")\n",
    "    print(f\"Macro Recall: {recall_macro:.4f}\")\n",
    "    print(f\"Macro F1-Score: {f1_macro:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Per-Class Metrics:\")\n",
    "    print(\"-\"*40)\n",
    "    for i, class_name in enumerate(CLASS_NAMES):\n",
    "        print(f\"{class_name:10} - Precision: {precision[i]:.4f}, Recall: {recall[i]:.4f}, F1: {f1[i]:.4f}\")\n",
    "    \n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=CLASS_NAMES))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=CLASS_NAMES,\n",
    "                yticklabels=CLASS_NAMES)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig('/content/results/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Simpan metrik ke file\n",
    "    metrics = {\n",
    "        'accuracy': float(accuracy),\n",
    "        'precision_macro': float(precision_macro),\n",
    "        'recall_macro': float(recall_macro),\n",
    "        'f1_macro': float(f1_macro),\n",
    "        'per_class_metrics': {}\n",
    "    }\n",
    "    \n",
    "    for i, class_name in enumerate(CLASS_NAMES):\n",
    "        metrics['per_class_metrics'][class_name] = {\n",
    "            'precision': float(precision[i]),\n",
    "            'recall': float(recall[i]),\n",
    "            'f1_score': float(f1[i])\n",
    "        }\n",
    "    \n",
    "    with open('/content/results/metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    return y_pred, y_pred_proba, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24e553b",
   "metadata": {},
   "source": [
    "# ==============================\n",
    "# 8. TESTING DENGAN 10 SAMPLE TERBAIK\n",
    "# =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edf79c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_predictions(model, X_test, y_test, top_n=10, samples_per_class=2):\n",
    "    \"\"\"Ambil sample terbaik untuk setiap kelas\"\"\"\n",
    "    \n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    best_samples = {}\n",
    "    \n",
    "    for class_idx, class_name in enumerate(CLASS_NAMES):\n",
    "        # Ambil indices untuk kelas ini\n",
    "        class_indices = np.where(y_test == class_idx)[0]\n",
    "        \n",
    "        # Filter hanya prediksi yang benar\n",
    "        correct_indices = class_indices[y_pred[class_indices] == class_idx]\n",
    "        \n",
    "        if len(correct_indices) == 0:\n",
    "            print(f\"Warning: No correct predictions for class {class_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Ambil confidence scores untuk prediksi yang benar\n",
    "        confidences = y_pred_proba[correct_indices, class_idx]\n",
    "        \n",
    "        # Sort berdasarkan confidence\n",
    "        sorted_indices = correct_indices[np.argsort(confidences)[::-1]]\n",
    "        \n",
    "        # Ambil top samples\n",
    "        top_samples = sorted_indices[:samples_per_class]\n",
    "        \n",
    "        best_samples[class_name] = {\n",
    "            'indices': top_samples,\n",
    "            'confidences': y_pred_proba[top_samples, class_idx]\n",
    "        }\n",
    "    \n",
    "    return best_samples\n",
    "\n",
    "def visualize_best_predictions(X_test, y_test, best_samples):\n",
    "    \"\"\"Visualisasi sample terbaik\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(len(CLASS_NAMES), 2, figsize=(8, len(CLASS_NAMES)*3))\n",
    "    \n",
    "    for row, (class_name, samples) in enumerate(best_samples.items()):\n",
    "        for col, (idx, conf) in enumerate(zip(samples['indices'], samples['confidences'])):\n",
    "            if len(CLASS_NAMES) == 1:\n",
    "                ax = axes[col]\n",
    "            else:\n",
    "                ax = axes[row, col]\n",
    "            \n",
    "            # Tampilkan gambar\n",
    "            img = X_test[idx]\n",
    "            if img.dtype != np.uint8:\n",
    "                img = (img * 255).astype(np.uint8)\n",
    "            \n",
    "            ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            ax.set_title(f'{class_name}\\nConfidence: {conf:.4f}')\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/content/results/best_predictions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9649b546",
   "metadata": {},
   "source": [
    "# ==============================\n",
    "# 9. MAIN EXECUTION PIPELINE\n",
    "# =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99578b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"HAND DETECTION WITH MOBILENETV2 SSD\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Preprocessing data\n",
    "    print(\"\\n1. PREPROCESSING DATA DENGAN MEDIAPIPE\")\n",
    "    print(\"-\"*40)\n",
    "    X, y = preprocess_dataset()\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"Error: No data loaded. Please check your dataset path!\")\n",
    "        return\n",
    "    \n",
    "    # Normalisasi data\n",
    "    X = X.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Step 2: Split data\n",
    "    print(\"\\n2. SPLITTING DATA\")\n",
    "    print(\"-\"*40)\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"Train set: {len(X_train)} samples\")\n",
    "    print(f\"Validation set: {len(X_val)} samples\")\n",
    "    print(f\"Test set: {len(X_test)} samples\")\n",
    "    \n",
    "    # Step 3: Data augmentation\n",
    "    print(\"\\n3. DATA AUGMENTATION\")\n",
    "    print(\"-\"*40)\n",
    "    X_train_aug, y_train_aug = augment_data(X_train, y_train, augment_factor=2)\n",
    "    \n",
    "    # Step 4: Training\n",
    "    print(\"\\n4. TRAINING MODEL\")\n",
    "    print(\"-\"*40)\n",
    "    model, history = train_model(X_train_aug, y_train_aug, X_val, y_val)\n",
    "    \n",
    "    # Step 5: Plot training history\n",
    "    print(\"\\n5. PLOTTING TRAINING HISTORY\")\n",
    "    print(\"-\"*40)\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Step 6: Evaluasi model\n",
    "    print(\"\\n6. EVALUASI MODEL\")\n",
    "    print(\"-\"*40)\n",
    "    y_pred, y_pred_proba, metrics = evaluate_model(model, X_test, y_test)\n",
    "    \n",
    "    # Step 7: Get best predictions\n",
    "    print(\"\\n7. ANALISIS 10 SAMPLE TERBAIK\")\n",
    "    print(\"-\"*40)\n",
    "    best_samples = get_best_predictions(model, X_test, y_test)\n",
    "    visualize_best_predictions(X_test, y_test, best_samples)    \n",
    "    \n",
    "    # Step 8: Save final model\n",
    "    print(\"\\n8. MENYIMPAN MODEL FINAL\")\n",
    "    print(\"-\"*40)\n",
    "    model.save('/content/models/final_hand_detection_model.h5')\n",
    "    \n",
    "    # Step 9: Summary\n",
    "    print(\"\\n9. RINGKASAN HASIL\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"✓ Dataset berhasil diproses: {len(X)} gambar\")\n",
    "    print(f\"✓ Model accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"✓ Precision (macro): {metrics['precision_macro']:.4f}\")\n",
    "    print(f\"✓ Recall (macro): {metrics['recall_macro']:.4f}\")\n",
    "    print(f\"✓ F1-Score (macro): {metrics['f1_macro']:.4f}\")\n",
    "    print(\"\\nFile yang tersedia untuk download:\")\n",
    "    print(\"- /content/models/final_hand_detection_model.h5 (Model terlatih)\")\n",
    "    print(\"- /content/realtime_hand_detection.py (Kode real-time)\")\n",
    "    print(\"- /content/results/ (Hasil evaluasi)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PIPELINE SELESAI!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Jalankan main pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# ==============================\n",
    "# BONUS: UTILITY FUNCTIONS\n",
    "# ==============================\n",
    "\n",
    "# def download_results():\n",
    "#     \"\"\"Download hasil training\"\"\"\n",
    "#     from google.colab import files\n",
    "    \n",
    "#     # Download model\n",
    "#     files.download('/content/models/final_hand_detection_model.h5')\n",
    "    \n",
    "#     # Download real-time code\n",
    "#     files.download('/content/realtime_hand_detection.py')\n",
    "    \n",
    "#     # Download metrics\n",
    "#     files.download('/content/results/metrics.json')\n",
    "    \n",
    "#     print(\"Semua file penting telah didownload!\")\n",
    "\n",
    "# Uncomment line di bawah untuk auto-download\n",
    "# download_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
